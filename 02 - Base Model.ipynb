{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"# Installing pyspark\n!pip install pyspark\nimport pyspark.sql.functions as f\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('Multiclass Classification').getOrCreate()\nimport pandas as pd\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-26T10:42:57.767727Z","iopub.execute_input":"2023-08-26T10:42:57.768177Z","iopub.status.idle":"2023-08-26T10:43:13.511946Z","shell.execute_reply.started":"2023-08-26T10:42:57.768144Z","shell.execute_reply":"2023-08-26T10:43:13.510508Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.4.1)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Loading Data","metadata":{}},{"cell_type":"code","source":"df = spark.read.options(delimeter=',', inferSchema=True, header=True).csv('/kaggle/input/customer/Train.csv')\ndf.show(5)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:35:19.663392Z","iopub.execute_input":"2023-08-26T12:35:19.663779Z","iopub.status.idle":"2023-08-26T12:35:20.107490Z","shell.execute_reply.started":"2023-08-26T12:35:19.663748Z","shell.execute_reply":"2023-08-26T12:35:20.106413Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"+------+------+------------+---+---------+-------------+---------------+--------------+-----------+-----+------------+\n|    ID|Gender|Ever_Married|Age|Graduated|   Profession|Work_Experience|Spending_Score|Family_Size|Var_1|Segmentation|\n+------+------+------------+---+---------+-------------+---------------+--------------+-----------+-----+------------+\n|462809|  Male|          No| 22|       No|   Healthcare|            1.0|           Low|        4.0|Cat_4|           D|\n|462643|Female|         Yes| 38|      Yes|     Engineer|           null|       Average|        3.0|Cat_4|           A|\n|466315|Female|         Yes| 67|      Yes|     Engineer|            1.0|           Low|        1.0|Cat_6|           B|\n|461735|  Male|         Yes| 67|      Yes|       Lawyer|            0.0|          High|        2.0|Cat_6|           B|\n|462669|Female|         Yes| 40|      Yes|Entertainment|           null|          High|        6.0|Cat_6|           A|\n+------+------+------------+---+---------+-------------+---------------+--------------+-----------+-----+------------+\nonly showing top 5 rows\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Handling Missing Data","metadata":{"execution":{"iopub.status.busy":"2023-08-26T10:18:39.948780Z","iopub.execute_input":"2023-08-26T10:18:39.949236Z","iopub.status.idle":"2023-08-26T10:18:40.084218Z","shell.execute_reply.started":"2023-08-26T10:18:39.949201Z","shell.execute_reply":"2023-08-26T10:18:40.082374Z"}}},{"cell_type":"code","source":"num_cols = []\nbinary_cols = ['Ever_Married', 'Gender', 'Graduated']\nmulti_cat_cols = ['Profession', 'Spending_Score', 'Var_1']\nfor cols in df.dtypes:\n    if cols[1] != 'string' and cols[0] != 'ID':\n        num_cols.append(cols[0])\nprint('Numerical Columns are:',*num_cols)\nprint('Binary categorical columns are:',*binary_cols)\nprint('Multilabel categorical columns are:',*multi_cat_cols)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:35:23.026676Z","iopub.execute_input":"2023-08-26T12:35:23.027379Z","iopub.status.idle":"2023-08-26T12:35:23.037262Z","shell.execute_reply.started":"2023-08-26T12:35:23.027294Z","shell.execute_reply":"2023-08-26T12:35:23.036145Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"Numerical Columns are: Age Work_Experience Family_Size\nBinary categorical columns are: Ever_Married Gender Graduated\nMultilabel categorical columns are: Profession Spending_Score Var_1\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Mode / Most frequent Values for Categorical columns:')\ndf.agg(*[f.mode(c).alias(c) for c in binary_cols+multi_cat_cols]).show()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:37:21.383141Z","iopub.execute_input":"2023-08-26T12:37:21.383535Z","iopub.status.idle":"2023-08-26T12:37:21.584723Z","shell.execute_reply.started":"2023-08-26T12:37:21.383505Z","shell.execute_reply":"2023-08-26T12:37:21.583524Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"Mode / Most frequent Values for Categorical columns:\n+------------+------+---------+----------+--------------+-----+\n|Ever_Married|Gender|Graduated|Profession|Spending_Score|Var_1|\n+------------+------+---------+----------+--------------+-----+\n|         Yes|  Male|      Yes|    Artist|           Low|Cat_6|\n+------------+------+---------+----------+--------------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Filling Model values for categorical columns\n#df = df.fillna( { 'Ever_Married':'Yes', 'Gender':'Male', 'Graduated':'Yes', 'Profession':'Artist','Spending_Score':'Low', 'Var_1':'Cat_6'} )","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:34:14.883656Z","iopub.execute_input":"2023-08-26T12:34:14.884047Z","iopub.status.idle":"2023-08-26T12:34:14.896946Z","shell.execute_reply.started":"2023-08-26T12:34:14.884016Z","shell.execute_reply":"2023-08-26T12:34:14.895777Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml import Pipeline\n\nind_b_cols = [i+\"_ind_b\" for i in binary_cols]\nind_m_cols = [i+\"_ind_m\" for i in multi_cat_cols]\nohe_op_cols = [i+\"_ohe\" for i in multi_cat_cols]\n\n# Imputing missing values in numerical columns using median strategy\nnum_imputer = Imputer(strategy='median',inputCols=num_cols, outputCols=num_cols)\n\n# String Indexing all string / categorical columns\nindexer = StringIndexer(inputCols= binary_cols+multi_cat_cols, outputCols=ind_b_cols+ind_m_cols, stringOrderType='alphabetAsc', handleInvalid ='keep')\n\n# String Indexing Segmentation (label) column\nlabel_index = StringIndexer(inputCol='Segmentation', outputCol='label',stringOrderType='alphabetAsc', handleInvalid='skip')\n\n# One Hot encoding multi label categorical columns\nencoder = OneHotEncoder(inputCols = ind_m_cols, outputCols = ohe_op_cols, dropLast = False)\n\n# Vector assembling all columns into a 'feature' vector\nassembler = VectorAssembler(inputCols = ind_b_cols+ohe_op_cols+num_cols, outputCol=\"features\")\n\n# Creating a pipeline for all preprocessing steps\nohe_pipe = Pipeline(stages=[num_imputer, indexer, encoder,label_index, assembler])\npipe_model = ohe_pipe.fit(df)\nprocessed_data = pipe_model.transform(df)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:35:29.654482Z","iopub.execute_input":"2023-08-26T12:35:29.654874Z","iopub.status.idle":"2023-08-26T12:35:30.614605Z","shell.execute_reply.started":"2023-08-26T12:35:29.654844Z","shell.execute_reply":"2023-08-26T12:35:30.613409Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"processed_data.select('Segmentation','label','features').show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:35:30.616330Z","iopub.execute_input":"2023-08-26T12:35:30.617246Z","iopub.status.idle":"2023-08-26T12:35:30.784438Z","shell.execute_reply.started":"2023-08-26T12:35:30.617204Z","shell.execute_reply":"2023-08-26T12:35:30.783163Z"},"trusted":true},"execution_count":141,"outputs":[{"name":"stdout","text":"+------------+-----+--------------------------------------------------------------------+\n|Segmentation|label|features                                                            |\n+------------+-----+--------------------------------------------------------------------+\n|D           |3.0  |(28,[1,8,15,20,25,26,27],[1.0,1.0,1.0,1.0,22.0,1.0,4.0])            |\n|A           |0.0  |(28,[0,2,5,13,20,25,26,27],[1.0,1.0,1.0,1.0,1.0,38.0,1.0,3.0])      |\n|B           |1.0  |(28,[0,2,5,15,22,25,26,27],[1.0,1.0,1.0,1.0,1.0,67.0,1.0,1.0])      |\n|B           |1.0  |(28,[0,1,2,10,14,22,25,27],[1.0,1.0,1.0,1.0,1.0,1.0,67.0,2.0])      |\n|A           |0.0  |(28,[0,2,6,14,22,25,26,27],[1.0,1.0,1.0,1.0,1.0,40.0,1.0,6.0])      |\n|C           |2.0  |(28,[0,1,3,13,22,25,27],[1.0,1.0,1.0,1.0,1.0,56.0,2.0])             |\n|C           |2.0  |(28,[1,2,8,15,22,25,26,27],[1.0,1.0,1.0,1.0,1.0,32.0,1.0,3.0])      |\n|D           |3.0  |(28,[2,8,15,22,25,26,27],[1.0,1.0,1.0,1.0,33.0,1.0,3.0])            |\n|D           |3.0  |(28,[0,2,5,15,23,25,27],[1.0,1.0,1.0,1.0,1.0,61.0,3.0])             |\n|C           |2.0  |(28,[0,2,3,13,22,25,26,27],[1.0,1.0,1.0,1.0,1.0,55.0,1.0,4.0])      |\n|A           |0.0  |(28,[2,5,15,22,25,26,27],[1.0,1.0,1.0,1.0,26.0,1.0,3.0])            |\n|D           |3.0  |(28,[1,8,15,20,25,26,27],[1.0,1.0,1.0,1.0,19.0,4.0,4.0])            |\n|D           |3.0  |(28,[7,15,19,25,27],[1.0,1.0,1.0,19.0,3.0])                         |\n|A           |0.0  |(28,[0,1,10,15,22,25,26,27],[1.0,1.0,1.0,1.0,1.0,70.0,1.0,1.0])     |\n|B           |1.0  |(28,[0,4,15,19,25,27],[1.0,1.0,1.0,1.0,58.0,1.0])                   |\n|C           |2.0  |(28,[8,15,17,25,26,27],[1.0,1.0,1.0,41.0,1.0,2.0])                  |\n|D           |3.0  |(28,[9,15,19,25,26,27],[1.0,1.0,1.0,32.0,9.0,5.0])                  |\n|B           |1.0  |(28,[1,8,15,22,25,26,27],[1.0,1.0,1.0,1.0,31.0,1.0,6.0])            |\n|B           |1.0  |(28,[0,1,2,6,13,22,25,26,27],[1.0,1.0,1.0,1.0,1.0,1.0,58.0,1.0,4.0])|\n|C           |2.0  |(28,[0,2,3,14,22,25,27],[1.0,1.0,1.0,1.0,1.0,79.0,1.0])             |\n+------------+-----+--------------------------------------------------------------------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"train, test = processed_data.randomSplit([0.8,0.2], seed=100)\nprint(\"There are %d Training samples and %d Test samples.\"%(train.count(), test.count()))","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:35:31.449670Z","iopub.execute_input":"2023-08-26T12:35:31.450109Z","iopub.status.idle":"2023-08-26T12:35:32.420912Z","shell.execute_reply.started":"2023-08-26T12:35:31.450046Z","shell.execute_reply":"2023-08-26T12:35:32.419748Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"There are 6468 Training samples and 1600 Test samples.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training Base Models","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression()\nlrModel = lr.fit(train)\n\npredictions = lrModel.transform(test)\npredictions.select('label','rawPrediction','probability','prediction').show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:35:33.504172Z","iopub.execute_input":"2023-08-26T12:35:33.504973Z","iopub.status.idle":"2023-08-26T12:35:36.707534Z","shell.execute_reply.started":"2023-08-26T12:35:33.504935Z","shell.execute_reply":"2023-08-26T12:35:36.706159Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"+-----+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\n|label|rawPrediction                                                                     |probability                                                                      |prediction|\n+-----+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\n|3.0  |[-0.6147911128455169,-0.9064661381698055,-0.7004923629984652,2.2217496140137873]  |[0.0507059809465124,0.037877938527409324,0.04654141776958071,0.8648746627564976] |3.0       |\n|3.0  |[0.3342172290181016,-0.8565404943383557,-0.6785431505505968,1.2008664158708506]   |[0.24714962550306213,0.0751312212294851,0.08976844709591769,0.5879507061715351]  |3.0       |\n|0.0  |[0.43170108762444726,0.1263663413707123,-0.8079809531668395,0.2499135241716801]   |[0.3496342911721422,0.2576373509373914,0.10121082171274311,0.2915175361777232]   |0.0       |\n|3.0  |[0.8422235070375096,0.12189096688542783,-1.3910502843311225,0.4269358104081853]   |[0.44367191725296296,0.21588650376016602,0.04755141624204226,0.2928901627448288] |0.0       |\n|1.0  |[-0.33776249109264567,1.047112867799441,2.060698905356714,-2.7700492820635096]    |[0.06215661237513733,0.24827389820950796,0.6841099173240267,0.00545957209132801] |2.0       |\n|1.0  |[-0.016505792126376706,0.40492637588033203,0.8199509750109919,-1.2083715587649468]|[0.19470514613475012,0.296758450861078,0.44941337007935356,0.05912303292481836]  |2.0       |\n|2.0  |[-0.59905621925405,0.39165959830068214,1.1230106009339487,-0.9156139799805811]    |[0.09982134839304585,0.26883502184131264,0.5586085124990109,0.07273511726663069] |2.0       |\n|3.0  |[-0.0034453015956331656,0.7161440016119542,0.6055681857502351,-1.318266885766556] |[0.1937710908047858,0.3979263035914641,0.3562707606599475,0.052031844943802565]  |1.0       |\n|2.0  |[-0.4976142318600548,1.1202890562875207,2.341000016875372,-2.963674841302838]     |[0.04306726983820314,0.2171669898899848,0.736108522456222,0.0036572178155900733] |2.0       |\n|0.0  |[0.5485637047116351,0.470078756612968,0.03413551345013821,-1.0527779747747417]    |[0.3671093323054541,0.3393984424899115,0.21947350533736973,0.07401871986726466]  |0.0       |\n|3.0  |[-0.8456843377335655,-0.8079411720539477,-0.32950265497001663,1.9831281647575303] |[0.04845094588166321,0.050314586577525994,0.08118524702295152,0.8200492205178593]|3.0       |\n|2.0  |[-0.5262109209858051,0.8837635125853203,2.1219169661331168,-2.479469557732632]    |[0.05163894959612113,0.21150542715120654,0.7295326415012022,0.007322981751470111]|2.0       |\n|3.0  |[0.7508499793595556,-0.28173691093410663,-1.596205993906731,1.1270929254812825]   |[0.34381558560458453,0.12242744085879463,0.03288607947600699,0.5008708940606137] |3.0       |\n|2.0  |[-0.1723398068951788,0.63704841831833,1.4232796656467745,-1.8879882770699261]     |[0.11964923772482719,0.2687959841643897,0.5900362147812602,0.021518563329522896] |2.0       |\n|0.0  |[-0.05539111432778343,0.3528159654333369,0.18167570482426365,-0.47910055592981715]|[0.22592444768883066,0.33981716428954994,0.2863650685432841,0.14789331947833523] |1.0       |\n|0.0  |[0.4967156966027676,0.29685849553805194,-0.15832190939156,-0.6352522827492595]    |[0.375845413108506,0.3077601427982452,0.19522257492187375,0.12117186917137511]   |0.0       |\n|0.0  |[-0.008519389977176162,0.7436936011316115,0.6455303065768949,-1.3807045177313304] |[0.18873065363007238,0.40042796059572044,0.36298829265508664,0.04785309311912055]|1.0       |\n|0.0  |[-0.5802711563820991,-0.4570657178863537,-0.07036821868790333,1.1077050929563563] |[0.10863931966715987,0.12288375657505221,0.18089854318475626,0.5875783805730317] |3.0       |\n|2.0  |[0.04619163503155399,0.6197207236745412,0.6805428675440761,-1.3464552262501712]   |[0.2037185290939484,0.361502287868656,0.38417205581992064,0.05060712721747482]   |2.0       |\n|3.0  |[-0.8387486982187472,-0.8030233316600479,-0.3185959520412094,1.960367981920005]   |[0.04963078891947689,0.05143591953582726,0.08349311151860869,0.8154401800260871] |3.0       |\n+-----+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------+\nonly showing top 20 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nlr_acc = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\nlr_f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\nlr_precision = evaluator.evaluate(predictions, {evaluator.metricName: \"precisionByLabel\"})\nprint('Below are the Test metrics:')\nprint('Accuracy: ', lr_acc)\nprint('F1 Score: ', lr_f1)\nprint('Precision: ', lr_precision)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:35:36.709441Z","iopub.execute_input":"2023-08-26T12:35:36.709781Z","iopub.status.idle":"2023-08-26T12:35:38.188771Z","shell.execute_reply.started":"2023-08-26T12:35:36.709751Z","shell.execute_reply":"2023-08-26T12:35:38.187400Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Below are the Test metrics:\nAccuracy:  0.524375\nF1 Score:  0.507622020909716\nPrecision:  0.424507658643326\n","output_type":"stream"}]},{"cell_type":"code","source":"model_list = []\nmodel_acc = []\nmodel_f1 = []\nmodel_list.append('Logistic Regression')\nmodel_acc.append(f'{lr_acc:.3f}')\nmodel_f1.append(f'{lr_f1:.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T11:41:52.144157Z","iopub.execute_input":"2023-08-26T11:41:52.144556Z","iopub.status.idle":"2023-08-26T11:41:52.150641Z","shell.execute_reply.started":"2023-08-26T11:41:52.144527Z","shell.execute_reply":"2023-08-26T11:41:52.149341Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nrfcModel = rfc.fit(train)\nrfc_predictions = rfcModel.transform(test)\n\nrfc_acc = evaluator.evaluate(rfc_predictions, {evaluator.metricName: \"accuracy\"})\nrfc_f1 = evaluator.evaluate(rfc_predictions, {evaluator.metricName: \"f1\"})\nrfc_precision = evaluator.evaluate(rfc_predictions, {evaluator.metricName: \"precisionByLabel\"})\nprint('Below are the Test metrics:')\nprint('Accuracy: ', rfc_acc)\nprint('F1 Score: ', rfc_f1)\nprint('Precision: ', rfc_precision)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:35:51.013306Z","iopub.execute_input":"2023-08-26T12:35:51.013696Z","iopub.status.idle":"2023-08-26T12:35:55.335681Z","shell.execute_reply.started":"2023-08-26T12:35:51.013664Z","shell.execute_reply":"2023-08-26T12:35:55.334758Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Below are the Test metrics:\nAccuracy:  0.52125\nF1 Score:  0.5086240900975929\nPrecision:  0.4263392857142857\n","output_type":"stream"}]},{"cell_type":"code","source":"model_list.append('Random Forest Classifier')\nmodel_acc.append(f'{rfc_acc:.3f}')\nmodel_f1.append(f'{rfc_f1:.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T11:45:15.142667Z","iopub.execute_input":"2023-08-26T11:45:15.143121Z","iopub.status.idle":"2023-08-26T11:45:15.148416Z","shell.execute_reply.started":"2023-08-26T11:45:15.143087Z","shell.execute_reply":"2023-08-26T11:45:15.147303Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import NaiveBayes\n\nnb = NaiveBayes()\nnbModel = nb.fit(train)\nnb_predictions = nbModel.transform(test)\n\nnb_acc = evaluator.evaluate(nb_predictions, {evaluator.metricName: \"accuracy\"})\nnb_f1 = evaluator.evaluate(nb_predictions, {evaluator.metricName: \"f1\"})\nnb_precision = evaluator.evaluate(nb_predictions, {evaluator.metricName: \"precisionByLabel\"})\nprint('Below are the Test metrics:')\nprint('Accuracy: ', nb_acc)\nprint('F1 Score: ', nb_f1)\nprint('Precision: ', nb_precision)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T11:46:59.736489Z","iopub.execute_input":"2023-08-26T11:46:59.736918Z","iopub.status.idle":"2023-08-26T11:47:01.657413Z","shell.execute_reply.started":"2023-08-26T11:46:59.736886Z","shell.execute_reply":"2023-08-26T11:47:01.656388Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"Below are the Test metrics:\nAccuracy:  0.46375\nF1 Score:  0.4409486093398237\nPrecision:  0.36363636363636365\n","output_type":"stream"}]},{"cell_type":"code","source":"model_list.append('Naive Bayes')\nmodel_acc.append(f'{nb_acc:.3f}')\nmodel_f1.append(f'{nb_f1:.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:03:44.439570Z","iopub.execute_input":"2023-08-26T12:03:44.439979Z","iopub.status.idle":"2023-08-26T12:03:44.444979Z","shell.execute_reply.started":"2023-08-26T12:03:44.439949Z","shell.execute_reply":"2023-08-26T12:03:44.444140Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.classification import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtcModel = dtc.fit(train)\ndtc_predictions = dtcModel.transform(test)\n\ndtc_acc = evaluator.evaluate(dtc_predictions, {evaluator.metricName: \"accuracy\"})\ndtc_f1 = evaluator.evaluate(dtc_predictions, {evaluator.metricName: \"f1\"})\ndtc_precision = evaluator.evaluate(dtc_predictions, {evaluator.metricName: \"precisionByLabel\"})\nprint('Below are the Test metrics:')\nprint('Accuracy: ', dtc_acc)\nprint('F1 Score: ', dtc_f1)\nprint('Precision: ', dtc_precision)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:03:08.223013Z","iopub.execute_input":"2023-08-26T12:03:08.223448Z","iopub.status.idle":"2023-08-26T12:03:11.354835Z","shell.execute_reply.started":"2023-08-26T12:03:08.223417Z","shell.execute_reply":"2023-08-26T12:03:11.353706Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Below are the Test metrics:\nAccuracy:  0.5\nF1 Score:  0.49654084500550116\nPrecision:  0.4296675191815857\n","output_type":"stream"}]},{"cell_type":"code","source":"model_list.append('Decision Tree')\nmodel_acc.append(f'{dtc_acc:.3f}')\nmodel_f1.append(f'{dtc_f1:.3f}')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:04:20.768157Z","iopub.execute_input":"2023-08-26T12:04:20.768541Z","iopub.status.idle":"2023-08-26T12:04:20.773745Z","shell.execute_reply.started":"2023-08-26T12:04:20.768512Z","shell.execute_reply":"2023-08-26T12:04:20.772817Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"### Comparing Metrics of all models","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\"Model\":model_list,\n             \"Accuracy\": model_acc,\n              'F1_Score': model_f1}).head()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T12:07:01.372375Z","iopub.execute_input":"2023-08-26T12:07:01.373138Z","iopub.status.idle":"2023-08-26T12:07:01.387730Z","shell.execute_reply.started":"2023-08-26T12:07:01.373099Z","shell.execute_reply":"2023-08-26T12:07:01.386641Z"},"trusted":true},"execution_count":118,"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"                      Model Accuracy F1_Score\n0       Logistic Regression    0.524    0.508\n1  Random Forest Classifier    0.521    0.509\n2               Naive Bayes    0.464    0.441\n3             Decision Tree    0.500    0.497","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>F1_Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Logistic Regression</td>\n      <td>0.524</td>\n      <td>0.508</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Random Forest Classifier</td>\n      <td>0.521</td>\n      <td>0.509</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Naive Bayes</td>\n      <td>0.464</td>\n      <td>0.441</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Decision Tree</td>\n      <td>0.500</td>\n      <td>0.497</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"- **Logistic Regression Has the Highest accuracy**\n- **Random forest has highest F1 score but a difference of only 0.001**\n- **Hence will Choose Logistic Regression for Cross Validation and further tuning**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}